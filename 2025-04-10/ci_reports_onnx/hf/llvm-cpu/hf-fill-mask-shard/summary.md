## Passing Summary

**TOTAL TESTS = 70**
|Stage|# Passing|% of Total|% of Attempted|
|--|--|--|--|
| Setup | 0 | 0.0% | 0.0% |
| IREE Compilation | 0 | 0.0% | 0.0% |
| Gold Inference | 0 | 0.0% | 0.0% |
| IREE Inference Invocation | 0 | 0.0% | 0.0% |
| Inference Comparison (PASS) | 0 | 0.0% | 0.0% |
## Fail Summary

**TOTAL TESTS = 70**
|Stage|# Failed at Stage|% of Total|
|--|--|--|
| Setup | 70 | 100.0% |
| IREE Compilation | 0 | 0.0% |
| Gold Inference | 0 | 0.0% |
| IREE Inference Invocation | 0 | 0.0% |
| Inference Comparison | 0 | 0.0% |
## Test Run Detail
Test was run with the following arguments:
Namespace(device='local-task', backend='llvm-cpu', target_chip='x86_64-linux-gnu', iree_compile_args=None, mode='cl-onnx-iree', torchtolinalg=False, stages=None, skip_stages=None, benchmark=False, load_inputs=False, groups='all', test_filter=None, testsfile='onnx_tests/models/external_lists/hf-model-shards/hf-fill-mask-shard.txt', tolerance=None, verbose=True, rundirectory='./test-onnx', no_artifacts=False, cleanup='3', report=True, report_file='reports/hf-fill-mask-shard.md', get_metadata=True)

| Test | Exit Status | Mean Benchmark Time (ms) | Notes |
|--|--|--|--|
| hf_albert-base-v2 | setup | None | |
| hf_albert-japanese-v2 | setup | None | |
| hf_ARBERTv2 | setup | None | |
| hf_astroBERT | setup | None | |
| hf_bert-base-5lang-cased | setup | None | |
| hf_bert-base-arabertv02 | setup | None | |
| hf_bert-base-cased | setup | None | |
| hf_bert-base-chinese | setup | None | |
| hf_bert-base-german-cased | setup | None | |
| hf_bert-base-indonesian-1.5G | setup | None | |
| hf_bert-base-italian-xxl-cased | setup | None | |
| hf_bert-base-italian-xxl-uncased | setup | None | |
| hf_bert-base-japanese | setup | None | |
| hf_bert-base-japanese-char | setup | None | |
| hf_bert-base-japanese-char-v2 | setup | None | |
| hf_bert-base-japanese-whole-word-masking | setup | None | |
| hf_bert-base-multilingual-cased | setup | None | |
| hf_bert-base-multilingual-uncased | setup | None | |
| hf_bert-base-portuguese-cased | setup | None | |
| hf_bert-base-spanish-wwm-uncased | setup | None | |
| hf_bert-base-uncased | setup | None | |
| hf_bert-kor-base | setup | None | |
| hf_bert-large-cased | setup | None | |
| hf_bert-large-portuguese-cased | setup | None | |
| hf_bert-large-uncased | setup | None | |
| hf_bertweet-base | setup | None | |
| hf_Bio_ClinicalBERT | setup | None | |
| hf_biobert-base-cased-v1.2 | setup | None | |
| hf_BiomedNLP-BiomedBERT-base-uncased-abstract | setup | None | |
| hf_BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext | setup | None | |
| hf_camembert-base | setup | None | |
| hf_ChemBERTa-77M-MLM | setup | None | |
| hf_chinese-roberta-wwm-ext | setup | None | |
| hf_codebert-base-mlm | setup | None | |
| hf_codebert-java | setup | None | |
| hf_codebert-python | setup | None | |
| hf_deberta-base | setup | None | |
| hf_deberta-v2-base-japanese | setup | None | |
| hf_deberta-v2-base-japanese-char-wwm | setup | None | |
| hf_deberta-v3-base | setup | None | |
| hf_deberta-v3-large | setup | None | |
| hf_deberta-v3-small | setup | None | |
| hf_deberta-v3-xsmall | setup | None | |
| hf_distilbert-base-cased | setup | None | |
| hf_distilbert-base-multilingual-cased | setup | None | |
| hf_distilbert-base-uncased | setup | None | |
| hf_distilroberta-base | setup | None | |
| hf_efficient-splade-VI-BT-large-doc | setup | None | |
| hf_efficient-splade-VI-BT-large-query | setup | None | |
| hf_esm2_t12_35M_UR50D | setup | None | |
| hf_esm2_t30_150M_UR50D | setup | None | |
| hf_esm2_t36_3B_UR50D | setup | None | |
| hf_esm2_t6_8M_UR50D | setup | None | |
| hf_IndicBERTv2-MLM-only | setup | None | |
| hf_legal-bert-base-uncased | setup | None | |
| hf_legal-bert-small-uncased | setup | None | |
| hf_mdeberta-v3-base | setup | None | |
| hf_opensearch-neural-sparse-encoding-doc-v2-distill | setup | None | |
| hf_phobert-base | setup | None | |
| hf_phobert-base-v2 | setup | None | |
| hf_prot_bert | setup | None | |
| hf_robbert-v2-dutch-base | setup | None | |
| hf_robeczech-base | setup | None | |
| hf_roberta-base | setup | None | |
| hf_roberta-large | setup | None | |
| hf_splade-cocondenser-ensembledistil | setup | None | |
| hf_splade-cocondenser-selfdistil | setup | None | |
| hf_Splade_PP_en_v1 | setup | None | |
| hf_wangchanberta-base-att-spm-uncased | setup | None | |
| hf_xlm-roberta-base | setup | None | |
